---
title: "01 Prosecutors"
author: "J Andres Gannon, Danil Gromakov, Katelyn Hess, and Yulia Krylova"
date: "May 2020"
output:
  html_document:
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
  html_notebook:
    toc: yes
    toc_float: yes
editor_options:
  chunk_output_type: inline
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(magrittr)
library(ggplot2)
```

This document loads, cleans, and processes newly collected data on prosecutor's appointment during the Trump administration. We collect data on their demographic information, employment history, education history, and membership in various organizations to identify patterns in how these prosecutors were chosen.

# Base data
## Load
```{r message = FALSE}
df <- googlesheets4::read_sheet(ss = "https://docs.google.com/spreadsheets/d/1hPOBOyTujUReQzpjZ9PqIhj23V5C74EygMnmXIi6BgI/edit?usp=sharing",
                                    sheet = "prosecutors_list")

df <- janitor::clean_names(df)
```

## Summarize basic info
```{r, fig.width = 16, fig.height = 16, warning = FALSE}
DT::datatable(df)
```

## SPARQL query
### Run query
We can add covariates of interest from the wikidata pages for each member. Info we can reliable get for each person includes sex, date of birth, place of birth, occupation, and educated at.

The output from R causes a bunch of messy string parsing. So it was copy-pasted into the query website to get a clean csv
```{r, eval = FALSE}
library(SPARQL)
library(ggplot2)

endpoint <- "https://query.wikidata.org/sparql"
query <- 'SELECT ?item ?itemLabel ?class ?classLabel ?sex_or_gender ?sex_or_genderLabel ?date_of_birth ?place_of_birth ?place_of_birthLabel ?occupation ?occupationLabel ?educated_at ?educated_atLabel WHERE {\n  VALUES ?item {\nwd:Q30508807
\nwd:Q32178229
\nwd:Q32171993
\nwd:Q38831261
\nwd:Q61750281
\nwd:Q39439933
\nwd:Q45916875
\nwd:Q48926431
\nwd:Q47457151
\nwd:Q16146008
\nwd:Q55238022
\nwd:Q55238023
\nwd:Q6230524
\nwd:Q45915219
\nwd:Q32059635
\nwd:Q53062078
\nwd:Q56284262
\nwd:Q54951622
\nwd:Q38864225
\nwd:Q28840361
\nwd:Q42308027
\nwd:Q48927544
\nwd:Q4865059
\nwd:Q56284266
\nwd:Q44532195
\nwd:Q41593180
\nwd:Q39515420
\nwd:Q40222043
\nwd:Q40234869
\nwd:Q48791963
\nwd:Q42422254
\nwd:Q40338033
\nwd:Q48673968
\nwd:Q48926975
\nwd:Q55238029
\nwd:Q39064553
\nwd:Q48675953
\nwd:Q42310535
\nwd:Q55238025
\nwd:Q16221406
\nwd:Q51741273
\nwd:Q39505579
\nwd:Q48926405
\nwd:Q40342099
\nwd:Q48977188
\nwd:Q55584805
\nwd:Q45914636
\nwd:Q48743453
\nwd:Q41536729
\nwd:Q43380464
\nwd:Q42319382
\nwd:Q5307301
\nwd:Q30519351
\nwd:Q66448890
\nwd:Q32034550
\nwd:Q40460795
\nwd:Q61941673
\nwd:Q48471887
\nwd:Q5233835
\nwd:Q48740794
\nwd:Q48688244
\nwd:Q64747678
\nwd:Q58008030
\nwd:Q51858469
\nwd:Q87001091
\nwd:Q33633456
\nwd:Q5300793
\nwd:Q38946601
\nwd:Q30453230
\nwd:Q47499109
\nwd:Q45713701
\nwd:Q46303027
\nwd:Q42331379
\nwd:Q30529752
\nwd:Q45714460
\nwd:Q55584793
\nwd:Q48926392
\nwd:Q63700125
\nwd:Q53063726
\nwd:Q43381318
\nwd:Q42346740
\nwd:Q47484556
\nwd:Q42353365
\nwd:Q42351529
\nwd:Q66819535
\nwd:Q47037424
\nwd:Q58972555
\nwd:Q42417575
\nwd:Q38929933
\nwd:Q5607717
\n  }
\n  ?item wdt:P31 ?class.\n  
OPTIONAL { ?item wdt:P21 ?sex_or_gender. }\n 
OPTIONAL { ?item wdt:P569 ?date_of_birth. }\n  
OPTIONAL { ?item wdt:P19 ?place_of_birth. }\n  
OPTIONAL { ?item wdt:P106 ?occupation. }\n  
OPTIONAL { ?item wdt:P69 ?educated_at. }\n  
SERVICE wikibase:label { bd:serviceParam wikibase:language "[AUTO_LANGUAGE],en". }\n}'
useragent <- paste("WDQS-Example", R.version.string)

qd <- SPARQL(endpoint,query,curl_args=list(useragent=useragent))
df <- qd$results
```

### Summarize info
```{r, message = FALSE}
df_wiki <- read.csv(file = paste0(here::here(), '/data/query.csv'))
```

Extract wikidata IDs from SPARQL query for easier merging
```{r}
df_wiki$id <- sub(".*entity/", "", df_wiki$item)
```

Subset wiki query to qualities of interest and do minor cleaning
```{r}
# Subset
df_wiki <- df_wiki %>% dplyr::select(c("id", "itemLabel", "sex_or_genderLabel", "date_of_birth", "place_of_birthLabel", "educated_atLabel"))
df_wiki <- unique(df_wiki)

# Fix DoB type
df_wiki$date_of_birth <- as.Date(df_wiki$date_of_birth)

# Add age var
age <- eeptools::age_calc(na.omit(df_wiki$date_of_birth), units = "years")
df_wiki$age <- NA
df_wiki$age[!is.na(df_wiki$date_of_birth)] <- age
df_wiki$age <- round(df_wiki$age)

# educated_at var should be in multiple columns so there's one row per prosecutor
df_wiki <- df_wiki %>% dplyr::group_by(id) %>%
       dplyr::mutate(rn = paste0("educated_atLabel", dplyr::row_number())) %>%
       tidyr::spread(rn, educated_atLabel)

# Examine
DT::datatable(df_wiki)
```

## Initial stats
### Sex or gender
```{r, fig.width=16, fig.height=10}
ggplot(df_wiki, aes(sex_or_genderLabel, ..count..)) + 
  geom_bar() + 
  labs(title = "Trump Prosecutors", x = "Sex or Gender", y = "Count", fill = "") + 
  geom_text(stat = 'count', aes(label = ..count..), vjust = -1, size = 6) +
  theme(plot.title = element_text(hjust = 0.5),panel.grid = element_blank(),
        text = element_text(size = 24))
```

### Age
```{r}
ggplot(df_wiki, aes(age, ..count..)) + 
  geom_histogram() + 
  labs(title = "Trump Prosecutors", x = "Age", y = "Count", fill = "") + 
  theme(plot.title = element_text(hjust = 0.5),panel.grid = element_blank(),
        text = element_text(size = 24))
```

## Merge wikidata with base
```{r, warning = FALSE}
df_full <- dplyr::left_join(df, df_wiki, by = c('wikidata_id' = 'id'))

# Only keep non-redundant rows
df_full <- df_full %>% dplyr::select(c("itemLabel", "district", "date_of_appointment", "barr_appointment", "sex_or_genderLabel", "date_of_birth", "place_of_birthLabel", "age")) %>%
  dplyr::rename(sex = sex_or_genderLabel, birthplace = place_of_birthLabel)
```

## Save
Save this version of the data as a new sheet on the original googledoc
```{r, message = FALSE}
df_full <- write.csv(df_full, paste0(here::here(), "/data/", "prosecutors_base.csv"))
```

# Employment
## Load
```{r}
work <- googlesheets4::read_sheet(ss = "https://docs.google.com/spreadsheets/d/1hPOBOyTujUReQzpjZ9PqIhj23V5C74EygMnmXIi6BgI/edit?usp=sharing",
                                    sheet = "prosecutors_employ",
                                  col_types = "cccccnnnc")

work <- janitor::clean_names(work)
```

## Clean
```{r}
# Check any notes
unique(work$notes)
work$notes <- NULL

# Check employer strings
sort(unique(work$employer))

# Fix start and end date
work$start_date[work$start_date == "Unknown"] <- NA
work$end_date[work$end_date == "Unknown"] <- NA
```

## Prep for network
```{r}
# Places of employment with the most unique attorneys
work_count <- work %>% dplyr::select(attorney, employer) %>%
  dplyr::distinct() %>%
  dplyr::group_by(employer) %>%
  dplyr::summarise(count = dplyr::n()) %>%
  dplyr::filter(count > 1)

DT::datatable(work_count)
```

## Save
````{r}
work <- work %>% dplyr::select(attorney, employer, firm_conservative_score, start_date, end_date)

work <- write.csv(work, paste0(here::here(), "/data/", "prosecutors_employment.csv"))
```

# School
## Load
```{r}
edu <- googlesheets4::read_sheet(ss = "https://docs.google.com/spreadsheets/d/1hPOBOyTujUReQzpjZ9PqIhj23V5C74EygMnmXIi6BgI/edit?usp=sharing",
                                    sheet = "prosecutors_edu",
                                    col_types = "cccccnnncc")

edu <- janitor::clean_names(edu)
```

## Clean
```{r}
## Collapse degrees
sort(unique(edu$degree))

edu$degree[edu$degree == "AA" |
             edu$degree == "AB" |
             edu$degree == "BA" |
             edu$degree == "BBA" |
             edu$degree == "BS"] <- "bachelors"
 
edu$degree[edu$degree == "LLM" |
             edu$degree == "MA" |
             edu$degree == "MMgt" |
             edu$degree == "MPA" |
             edu$degree == "MPhil" |
             edu$degree == "MS"] <- "masters"

edu$degree[edu$degree == "JD"] <- "law"

sort(unique(edu$degree))

# Accept undergrad LAC rankings as normal, since we're not using them for conservative rankings anyway
edu$undergrad_rank <- stringr::str_replace(edu$undergrad_rank, '\\*', '')
```

## Prep for network
```{r}
# Make wide
edu_clean <- edu %>% dplyr::select(item_label, educated_at_label, degree) %>%
  tidyr::spread(degree, educated_at_label) %>%
  dplyr::select(item_label, bachelors, masters, law)

edu_merge <- edu %>% dplyr::select(educated_at_label, law_rank, law_conservative_score) %>%
  dplyr::distinct()

edu_all <- dplyr::left_join(edu_clean, edu_merge, by = c("law" = "educated_at_label")) %>%
  dplyr::distinct()

# Clean each school col
sort(unique(edu_all$bachelors))
sort(unique(edu_all$masters))
sort(unique(edu_all$law))
```

## Results
```{r}
DT::datatable(edu_all)

GGally::ggpairs(edu_all[ , c('law_rank', 'law_conservative_score')], columnLabels = c("Law School Rank", "Law School Conservatism"))
```

## Save
```{r}
edu <- write.csv(edu_all, paste0(here::here(), "/data/", "prosecutors_edu.csv"))
```

# Memberships
## Load
```{r}
memb <- googlesheets4::read_sheet(ss = "https://docs.google.com/spreadsheets/d/1hPOBOyTujUReQzpjZ9PqIhj23V5C74EygMnmXIi6BgI/edit?usp=sharing",
                                    sheet = "prosecutors_memb")

memb <- janitor::clean_names(memb)
```

## Prep for network
```{r}
# Make wide
memb_clean <- memb %>% dplyr::select(district, attorney, political_affiliation, dplyr::ends_with("_tie"))
```

## Results
```{r}
DT::datatable(memb_clean)

memb_upset <- memb %>% dplyr::select(dplyr::ends_with("_tie")) %>%
  dplyr::rename("Federalist Society" = federalist_society_tie, "Heritage Foundation" = heritage_foundation_tie, "NRA" = national_rifle_association_tie, "American Center for Law and Justice" = american_center_for_law_and_justice_tie, "Republican National Lawyers Association" = republican_national_lawyers_association_tie, "Christian Legal Society" = christian_legal_society_bar_association_tie)

memb_upset <- as.data.frame(sapply(memb_upset, as.numeric))
memb_upset[is.na(memb_upset)] <- 0

UpSetR::upset(memb_upset,
      number.angles = 0, point.size = 3.5, line.size = 2, text.scale = 2,
      mainbar.y.label = "Number of Prosecutors", order.by = "freq")
```

# Final
## Load all cleaned
```{r}
# Load
base <- read.csv(paste0(here::here(), "/data/", "prosecutors_base.csv"))
employ <- read.csv(paste0(here::here(), "/data/", "prosecutors_employment.csv"))
edu <- read.csv(paste0(here::here(), "/data/", "prosecutors_edu.csv"))
memb <- read.csv(paste0(here::here(), "/data/", "prosecutors_membership.csv"))
```

## Clean
```{r}
colnames(base)
base <- base %>% dplyr::select(-c("X")) %>%
  dplyr::rename(person = itemLabel)

colnames(employ)
employ <- employ %>% dplyr::select(-c("X")) %>%
  dplyr::rename(person = attorney)

colnames(edu)
edu <- edu %>% dplyr::select(-c("X")) %>%
  dplyr::rename(person = item_label)

colnames(memb)
memb <- memb %>% dplyr::select(-c("X")) %>%
  dplyr::rename(person = attorney)
```

## Merge
Since the only one with more columns is employment, we merge everything into that
```{r}
full <- dplyr::left_join(employ, base)
full <- dplyr::left_join(full, edu)
full <- dplyr::left_join(full, memb)

full <- full %>% dplyr::distinct()
```

## Save
```{r}
write.csv(full, paste0(here::here(), "/data/", "prosecutors_final.csv"))
```